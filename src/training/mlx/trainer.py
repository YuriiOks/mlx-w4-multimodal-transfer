# Multimodal Transfer Learning / Image Captioning
# File: src/training/mlx/trainer.py
# Copyright (c) 2025 Dropout Disco Team (Yurii, Artemis, Nnamdi, Kenton)
# Description: MLX training/evaluation loop logic.
# Created: 2025-05-05
# Updated: 2025-05-08

import math
import os
import sys  # Added for sys.path manipulation
import time
from pathlib import Path
from typing import Any, Callable, Dict, List, Optional, Tuple

import mlx.core as mx
import mlx.nn as nn
import mlx.optimizers as optim  # Use optim alias for consistency
import numpy as np
from tqdm import tqdm

from src.common.constants import PAD_TOKEN_ID
from src.training.mlx.checkpoint import (  # Added for train_model_mlx
    save_checkpoint_mlx,
)
from utils import logger

# --- Add project root for imports ---
script_dir = os.path.dirname(os.path.abspath(__file__))
project_root = Path(script_dir).parent.parent.parent  # Go up three levels
if str(project_root) not in sys.path:
    print(f"üöÇ [trainer_mlx.py] Adding project root: {project_root}")
    sys.path.insert(0, str(project_root))

# --- Import TF for tensor-to-PIL conversion ---
try:
    import torch  # Need torch to check instance type
    import torchvision.transforms.functional as TF

    TORCHVISION_AVAILABLE = True
except ImportError:
    logger.warning(
        "‚ö†Ô∏è Torchvision not available. Image conversion in trainer might fail."
    )
    TORCHVISION_AVAILABLE = False

    # Define dummy TF and torch if not available to prevent runtime errors on attribute access
    class DummyTF:
        @staticmethod
        def to_pil_image(img):
            raise ImportError("Torchvision not available for to_pil_image")

    TF = DummyTF()

    class DummyTorch:
        Tensor = type(None)  # So isinstance(obj, torch.Tensor) is False

        def cpu(self):  # Dummy cpu method for tensor
            return self

    torch = DummyTorch()


# W&B Import Handling
try:
    import wandb
except ImportError:
    logger.warning("‚ö†Ô∏è wandb not installed. W&B logging disabled.")
    wandb = None


# --- Loss & Accuracy Calculation Function (Traced by value_and_grad) ---
# --- Loss & Accuracy Calculation Function (Traced by value_and_grad) ---
def calculate_loss_acc_mlx(
    model: nn.Module,
    images_pt: torch.Tensor,  # Expect PyTorch Tensor from DataLoader
    targets_full_mx: mx.array,  # Expect MLX array for targets
    phase: int = 1,  # Default to phase 1, but P3 will override
    pad_token_id: int = PAD_TOKEN_ID,
    is_encoder_decoder: bool = True,  # Assume True for captioning
) -> Tuple[mx.array, mx.array]:
    """
    Calculates loss and accuracy for a batch. Designed to be JIT-compiled
    with value_and_grad.

    Args:
        model: The MLX model instance.
        images_pt: Batch of images as PyTorch Tensors (B, C, H, W).
        targets_full_mx: Full target sequences as MLX arrays (B, T).
        phase (int): Current phase (used to determine model type/logic).
        pad_token_id (int): ID of the padding token.
        is_encoder_decoder (bool): Flag for model type.

    Returns:
        Tuple[mx.array, mx.array]: Batch loss and batch accuracy (per token).
    """
    # --- Convert images: PT Tensor -> List[PIL] ---
    try:
        if TORCHVISION_AVAILABLE and isinstance(images_pt, torch.Tensor):
            try:
                images_pil = [TF.to_pil_image(img.cpu()) for img in images_pt]
            except Exception as e:
                logger.error(f"‚ùå Failed converting PT tensor to PIL list: {e}")
                return mx.array(5.0), mx.array(0.1)
        elif isinstance(images_pt, list):  # If already a list of PIL
            images_pil = images_pt
        else:
            logger.error(
                f"‚ùå Image input is not a PT Tensor or List: {type(images_pt)}"
            )
            return mx.array(5.0), mx.array(0.1)
    except Exception as e:
        logger.error(f"‚ùå Error handling input images: {e}")
        return mx.array(5.0), mx.array(0.1)

    # Main calculation with comprehensive error handling
    try:
        if is_encoder_decoder:
            # Split input sequence into decoder input and target
            decoder_input_ids = targets_full_mx[:, :-1]  # All but last token
            decoder_target_ids = targets_full_mx[:, 1:]  # All but first token

            # Forward pass with robust error handling
            try:
                logits = model(images_pil, decoder_input_ids)
            except Exception as e:
                logger.warning(f"‚ö†Ô∏è Model forward pass failed: {e}")
                # Return fallback values to continue training
                return mx.array(5.0), mx.array(0.1)

            # If we made it here, we have logits - calculate loss and accuracy
            try:
                vocab_size = logits.shape[-1]

                # Compute loss with masking
                loss_per_token = nn.losses.cross_entropy(
                    logits.reshape(-1, vocab_size),
                    decoder_target_ids.reshape(-1),
                    reduction="none",
                )

                # Create mask to ignore padding tokens
                mask = decoder_target_ids.reshape(-1) != pad_token_id

                # Apply mask and calculate average loss
                mask_sum = mask.sum()
                if mask_sum.item() > 0:
                    masked_loss = (loss_per_token * mask).sum() / mask_sum
                else:
                    masked_loss = mx.array(0.0)

                # Calculate accuracy
                preds = mx.argmax(logits, axis=-1)
                correct_preds = preds == decoder_target_ids
                masked_correct = correct_preds * mask.reshape(preds.shape)
                if mask_sum.item() > 0:
                    accuracy = masked_correct.sum() / mask_sum
                else:
                    accuracy = mx.array(0.0)

                return masked_loss, accuracy

            except Exception as e:
                logger.warning(f"‚ö†Ô∏è Loss/accuracy calculation failed: {e}")
                return mx.array(5.0), mx.array(0.1)

        else:  # Classification case (not relevant for this project)
            try:
                logits = model(images_pil)
                loss = nn.losses.cross_entropy(
                    logits, targets_full_mx.reshape(-1), reduction="mean"
                )
                preds = mx.argmax(logits, axis=-1)
                accuracy = mx.mean(preds == targets_full_mx.reshape(-1))
                return loss, accuracy
            except Exception as e:
                logger.warning(
                    f"‚ö†Ô∏è Classification loss calculation failed: {e}"
                )
                return mx.array(5.0), mx.array(0.1)

    except Exception as e:
        logger.error(f"‚ùå Unhandled error in loss calculation: {e}")
        # Always return some values so training can continue
        return mx.array(5.0), mx.array(0.1)


# --- Epoch Training Function ---
def train_epoch_mlx(
    model: nn.Module,
    dataloader: Any,  # Can be PyTorch DataLoader
    optimizer: optim.Optimizer,
    epoch_num: int,
    total_epochs: int,
    log_frequency: int = 100,
    gradient_clipping: Optional[float] = 1.0,
    is_encoder_decoder: bool = False,  # Should be True for captioning
    pad_token_id: int = PAD_TOKEN_ID,
    wandb_run: Optional[Any] = None,
) -> float:
    model.train()
    total_loss = 0.0
    num_batches = len(dataloader)
    if num_batches == 0:
        logger.warning("Train dataloader empty.")
        return 0.0

    loss_and_grad_fn = nn.value_and_grad(model, calculate_loss_acc_mlx)

    data_iterator = tqdm(
        dataloader,
        desc=f"Epoch {epoch_num+1}/{total_epochs} Train",
        leave=False,
        unit="batch",
        ncols=100,
    )

    for batch_idx, batch_data in enumerate(data_iterator):
        images_pt = batch_data["pixel_values"]
        input_ids_pt = batch_data["input_ids"]
        input_ids_mx = mx.array(input_ids_pt.numpy())

        try:
            (loss, acc), grads = loss_and_grad_fn(
                model,
                images_pt,
                input_ids_mx,
                phase=3,
                pad_token_id=pad_token_id,
                is_encoder_decoder=is_encoder_decoder,
            )

            # Direct optimizer update only; no fallback logic
            optimizer.update(model, grads)
            mx.eval(model.parameters(), loss, acc)

            batch_loss_val = loss.item()
            batch_acc_val = acc.item()
            total_loss += batch_loss_val
            data_iterator.set_postfix(
                loss=f"{batch_loss_val:.4f}", acc=f"{batch_acc_val*100:.2f}%"
            )

            if (
                wandb is not None
                and wandb_run is not None
                and (batch_idx + 1) % log_frequency == 0
            ):
                global_step = epoch_num * num_batches + batch_idx
                log_dict = {
                    "batch_loss_mlx": batch_loss_val,
                    "batch_accuracy_mlx": batch_acc_val * 100.0,
                    "learning_rate_mlx": optimizer.learning_rate.item(),
                    "epoch_fractional_mlx": epoch_num
                    + ((batch_idx + 1) / num_batches),
                    "global_step_mlx": global_step,
                }
                try:
                    wandb_run.log(log_dict)
                except Exception as e_wandb:
                    logger.warning(
                        f"‚ö†Ô∏è Failed to log batch metrics to W&B: {e_wandb}"
                    )

        except Exception as e_batch:
            logger.error(
                f"‚ùå Unrecoverable error in training batch {batch_idx}: {e_batch}",
                exc_info=True,
            )
            raise e_batch

    average_loss = total_loss / num_batches if num_batches > 0 else 0.0
    return average_loss


# --- Evaluation Function ---
def evaluate_model_mlx(
    model: nn.Module,
    dataloader: Any,  # Can be PyTorch DataLoader
    batch_size: int,
    phase: int = 1,  # Default to phase 1
    pad_token_id: int = PAD_TOKEN_ID,
    is_encoder_decoder: bool = False,
) -> Dict[str, float]:
    model.eval()
    total_loss = 0.0
    total_correct_tokens = 0.0  # Use float for accumulation
    total_valid_tokens = 0

    # Try to get dataset length for accurate num_batches
    try:
        num_samples = len(dataloader.dataset)
        num_batches = math.ceil(num_samples / batch_size)
    except (
        AttributeError
    ):  # If dataloader has no .dataset (e.g. simple iterator)
        num_batches = len(dataloader)  # Fallback

    if num_batches == 0:
        return {"val_loss": 0.0, "val_accuracy": 0.0}

    logger.info("üß™ Starting MLX evaluation...")
    data_iterator = tqdm(
        dataloader,
        desc="Evaluating",
        leave=False,
        unit="batch",
        total=num_batches if num_batches > 0 else None,
    )

    for batch_data in data_iterator:
        images_pt = batch_data["pixel_values"]
        input_ids_pt = batch_data["input_ids"]
        input_ids_mx = mx.array(input_ids_pt.numpy())

        loss, acc = calculate_loss_acc_mlx(
            model,
            images_pt,
            input_ids_mx,
            phase=phase,
            pad_token_id=pad_token_id,
            is_encoder_decoder=is_encoder_decoder,
        )
        mx.eval(loss, acc)  # Ensure computation

        current_batch_valid_tokens = 0
        if is_encoder_decoder:
            targets = input_ids_mx[:, 1:]  # Targets for accuracy
            mask = targets != pad_token_id
            current_batch_valid_tokens = mask.sum().item()
        else:  # Simple classification
            targets = input_ids_mx
            current_batch_valid_tokens = (
                targets.size
            )  # Batch size if no padding

        if current_batch_valid_tokens > 0:
            total_loss += loss.item() * current_batch_valid_tokens
            total_correct_tokens += (
                acc.item() * current_batch_valid_tokens
            )  # acc is already per token
            total_valid_tokens += current_batch_valid_tokens

        data_iterator.set_postfix(
            loss=f"{loss.item():.4f}", acc=f"{acc.item()*100:.2f}%"
        )

    avg_loss = (
        total_loss / total_valid_tokens if total_valid_tokens > 0 else 0.0
    )
    avg_accuracy = (
        (total_correct_tokens / total_valid_tokens) * 100.0
        if total_valid_tokens > 0
        else 0.0
    )

    logger.info(
        f"üß™ MLX Eval finished. Loss: {avg_loss:.4f}, "
        f"Acc: {avg_accuracy:.2f}% ({int(total_correct_tokens)}/{int(total_valid_tokens)})"
    )
    return {"val_loss": avg_loss, "val_accuracy": avg_accuracy}


# --- Main Training Orchestrator Function ---
def train_model_mlx(
    model: nn.Module,
    train_dataloader: Any,  # PyTorch DataLoader
    val_dataloader: Optional[Any],  # PyTorch DataLoader
    optimizer: optim.Optimizer,
    target_epochs: int,
    start_epoch: int,
    metrics_history: Dict[str, List[float]],
    model_save_dir: Path,
    config: Dict,
    run_name: str,
    lr_scheduler: Optional[Callable] = None,
    save_every: int = 1,
    pad_token_id: int = PAD_TOKEN_ID,
    is_encoder_decoder: bool = False,
    wandb_run: Optional[Any] = None,
    gradient_clipping: Optional[float] = 1.0,
) -> Dict[str, List[float]]:
    logger.info(f"üöÄ Starting/Resuming MLX Training: Run='{run_name}'")
    logger.info(
        f"   Target Epochs: {target_epochs}, Starting from Epoch: {start_epoch}"
    )

    model_cfg_to_save = config.get("model", {})
    dataset_cfg_to_save = config.get("dataset", {})
    tokenizer_cfg_to_save = config.get("tokenizer", {})

    # Determine val_batch_size
    if (
        val_dataloader
        and hasattr(val_dataloader, "batch_size")
        and val_dataloader.batch_size is not None
    ):
        val_batch_size = val_dataloader.batch_size
    else:  # Fallback from config or default
        val_batch_size = config.get("evaluation", {}).get(
            "batch_size", config.get("training", {}).get("batch_size", 32)
        )

    best_val_accuracy = 0.0
    if metrics_history.get("val_accuracy"):
        valid_accs = [
            acc
            for acc in metrics_history["val_accuracy"]
            if not (isinstance(acc, float) and math.isnan(acc))
        ]
        if valid_accs:
            best_val_accuracy = max(valid_accs)

    for epoch in range(start_epoch, target_epochs):
        epoch_start_time = time.time()
        avg_train_loss = train_epoch_mlx(
            model=model,
            dataloader=train_dataloader,
            optimizer=optimizer,
            epoch_num=epoch,
            total_epochs=target_epochs,
            log_frequency=config.get("training", {}).get("log_frequency", 100),
            gradient_clipping=gradient_clipping,
            is_encoder_decoder=is_encoder_decoder,
            pad_token_id=pad_token_id,
            wandb_run=wandb_run,
        )
        metrics_history["avg_train_loss"].append(avg_train_loss)

        val_metrics = {}
        if val_dataloader:
            val_metrics = evaluate_model_mlx(
                model=model,
                dataloader=val_dataloader,
                batch_size=val_batch_size,
                phase=config.get("training", {}).get(
                    "phase", 3
                ),  # Default to phase 3 for eval
                pad_token_id=pad_token_id,
                is_encoder_decoder=is_encoder_decoder,
            )
            metrics_history["val_loss"].append(
                val_metrics.get("val_loss", float("nan"))
            )
            metrics_history["val_accuracy"].append(
                val_metrics.get("val_accuracy", float("nan"))
            )
        else:
            metrics_history["val_loss"].append(float("nan"))
            metrics_history["val_accuracy"].append(float("nan"))

        epoch_end_time = time.time()
        epoch_duration = epoch_end_time - epoch_start_time
        current_lr = optimizer.learning_rate.item()
        if lr_scheduler is not None:
            new_lr_val = lr_scheduler(
                current_lr
            )  # Assumes scheduler returns new LR
            optimizer.learning_rate = mx.array(new_lr_val)
            mx.eval(optimizer.state)
            current_lr = new_lr_val
        metrics_history["learning_rate"].append(current_lr)

        log_str = (
            f"‚úÖ Epoch {epoch+1:02d}/{target_epochs} | "
            f"Train Loss: {avg_train_loss:.4f} | "
        )
        if val_metrics:
            log_str += (
                f"Val Loss: {val_metrics['val_loss']:.4f} | "
                f"Val Acc: {val_metrics['val_accuracy']:.2f}% | "
            )
        log_str += f"LR: {current_lr:.2e} | " f"Time: {epoch_duration:.2f}s"
        logger.info(log_str)

        if wandb is not None and wandb_run is not None:
            try:
                wandb_log = {
                    "epoch": epoch + 1,
                    "avg_train_loss": avg_train_loss,
                    "learning_rate": current_lr,
                    "epoch_time_sec": epoch_duration,
                    **val_metrics,
                }
                wandb_run.log(wandb_log)
            except Exception as e:
                logger.error(f"‚ùå W&B epoch log failed: {e}")

        current_val_acc = val_metrics.get("val_accuracy", -1.0)
        is_best = current_val_acc > best_val_accuracy
        if is_best:
            best_val_accuracy = current_val_acc
            logger.info(
                f"üèÜ New best val_accuracy: {best_val_accuracy:.2f}% "
                f"at Epoch {epoch+1}"
            )
        if (
            (epoch + 1) % save_every == 0
            or epoch == target_epochs - 1
            or is_best
        ):
            save_checkpoint_mlx(
                model,
                optimizer,
                epoch,
                metrics_history,
                model_save_dir,
                is_best,
                model_cfg_to_save,
                dataset_cfg_to_save,
                tokenizer_cfg_to_save,
            )

    logger.info("üèÅ MLX Training finished for this run.")
    return metrics_history


# --- Test Block ---
if __name__ == "__main__":
    from PIL import Image

    # Dummy Tokenizer definition
    class DummyTokenizer:
        def __init__(self, vocab_size=50, pad_token_id=0):
            self.vocab_size = vocab_size
            self.pad_token_id = pad_token_id

        def __call__(self, texts, **kwargs):
            # Use kwargs to satisfy linter (unused parameter)
            _ = kwargs
            return {
                "input_ids": [
                    np.random.randint(0, self.vocab_size, (10,)).tolist()
                    for _ in texts
                ]
            }

        def decode(self, ids):
            # Use ids to satisfy linter (unused parameter)
            _ = ids
            return "dummy decoded text"

    # Use fallback logger if main one failed
    if "logger" not in globals() or logger is None:
        import logging

        logging.basicConfig(level=logging.INFO)
        logger = logging.getLogger("TrainerMLXTest")
        # Re-init TORCHVISION_AVAILABLE and torch/TF if logger was missing (implies script didn't run from top)
        try:
            import torchvision.transforms.functional as TF
        except ImportError:
            TORCHVISION_AVAILABLE = False

            class DummyTF:
                @staticmethod
                def to_pil_image(img):
                    # Use img to satisfy linter
                    return None

            TF = DummyTF()

            class DummyTorch:
                Tensor = type(None)

                def cpu(self):
                    return self

            torch = DummyTorch()

    logger.info("üß™ Running trainer_mlx.py basic checks...")
    PAD_TOKEN_ID = 0  # Define for test scope

    # --- Dummy Model for P1/P2 (Classification) ---
    class DummyP1P2Model(nn.Module):
        def __init__(self, num_classes=10, embed_dim=32):
            super().__init__()
            self.fc = nn.Linear(embed_dim, num_classes)
            self.dummy_input_proj = nn.Linear(
                3 * 32 * 32, embed_dim
            )  # Example: flat image

        def __call__(self, images_pil_list):  # Expects List[PIL]
            _ = len(images_pil_list)  # Used for batch size tracking
            processed_images = []  # Will store processed image tensors
            for img in images_pil_list:  # Dummy processing
                img_arr = mx.array(
                    np.array(img.resize((32, 32))).astype(np.float32) / 255.0
                )
                processed_images.append(img_arr.flatten())

            if not processed_images:  # Handle empty list
                dummy_output_shape = (
                    0,
                    self.fc.weight.shape[0],
                )  # (0, num_classes)
                return mx.zeros(dummy_output_shape)

            image_tensors = mx.stack(processed_images)  # (B, 3*32*32)
            x = self.dummy_input_proj(image_tensors)
            return self.fc(x)

    # --- Dummy Model for P3 (Encoder-Decoder) ---
    class DummyP3Model(nn.Module):
        def __init__(self, vocab_size=50, embed_dim=32):
            super().__init__()
            self.vocab_size = vocab_size
            self.embed_dim = embed_dim
            # Simulate a vision encoder part
            self.vision_projection = nn.Linear(
                3 * 64 * 64, embed_dim
            )  # Flat image to embed_dim
            # Decoder parts
            self.decoder_embedding = nn.Embedding(vocab_size, embed_dim)
            self.decoder_rnn = nn.LSTM(
                embed_dim, embed_dim
            )  # Simple LSTM decoder
            self.output_projection = nn.Linear(embed_dim, vocab_size)

        def encode(self, images_pil_list: List[Image.Image]) -> mx.array:
            processed_images = []
            for img in images_pil_list:
                img_arr = mx.array(
                    np.array(img.resize((64, 64))).astype(np.float32) / 255.0
                )
                processed_images.append(img_arr.flatten())
            image_tensors = mx.stack(processed_images)  # (B, 3*64*64)
            # Return a single "memory" vector per image
            return self.vision_projection(image_tensors)  # (B, embed_dim)

        def decode(
            self, decoder_input_ids: mx.array, memory: mx.array
        ) -> mx.array:
            # decoder_input_ids: (B, SeqLen)
            # memory: (B, embed_dim) - encoder output
            embedded_tokens = self.decoder_embedding(
                decoder_input_ids
            )  # (B, SeqLen, embed_dim)

            # LSTM expects (SeqLen, B, embed_dim) if batch_first=False (default)
            # Or (B, SeqLen, embed_dim) if batch_first=True
            # MLX LSTM is (seq_len, batch, input_size)
            lstm_input = mx.transpose(
                embedded_tokens, (1, 0, 2)
            )  # (SeqLen, B, embed_dim)

            # --- üëá REMOVED explicit hidden state initialization ---
            # h0 = mx.expand_dims(memory, axis=0)
            # c0 = mx.zeros_like(h0)
            # lstm_output, _ = self.decoder_rnn(lstm_input, (h0, c0)) # OLD
            lstm_output, _ = self.decoder_rnn(
                lstm_input
            )  # NEW - let LSTM use default zero state
            # --- End Correction ---

            # Transpose back to (B, SeqLen, embed_dim)
            lstm_output_transposed = mx.transpose(lstm_output, (1, 0, 2))
            return lstm_output_transposed

        def __call__(
            self,
            images_pil_list: List[Image.Image],
            decoder_input_ids: mx.array,
        ) -> mx.array:
            memory = self.encode(images_pil_list)  # (B, embed_dim)
            # Decoder needs memory replicated for each token step if used directly in attention
            # For LSTM, memory is used for initial state.
            decoded_features = self.decode(
                decoder_input_ids, memory
            )  # (B, SeqLen, embed_dim)
            logits = self.output_projection(
                decoded_features
            )  # (B, SeqLen, vocab_size)
            return logits

    dummy_tokenizer = DummyTokenizer(pad_token_id=PAD_TOKEN_ID)
    dummy_batch_size = 2

    logger.info("\n--- Testing P1/P2 loss/acc calc ---")
    try:
        dummy_model_p1 = DummyP1P2Model(num_classes=10, embed_dim=32)
        mx.eval(dummy_model_p1.parameters())  # Ensure params are created
        # Create PT Tensors for images_pt as expected by calculate_loss_acc_mlx
        dummy_images_p1_pt = (
            torch.randn(dummy_batch_size, 3, 32, 32)
            if TORCHVISION_AVAILABLE
            else [Image.new("RGB", (32, 32)) for _ in range(dummy_batch_size)]
        )  # Fallback to PIL
        dummy_labels_p1 = mx.array(
            np.random.randint(0, 10, (dummy_batch_size,))
        )

        loss, acc = calculate_loss_acc_mlx(
            dummy_model_p1,
            dummy_images_p1_pt,
            dummy_labels_p1,
            phase=1,
            is_encoder_decoder=False,
        )
        mx.eval(loss, acc)
        logger.info(
            f"P1 Dummy Loss: {loss.item():.4f}, Acc: {acc.item()*100:.2f}%"
        )
        logger.info("‚úÖ P1 calc OK.")
    except Exception as e:
        logger.error(f"‚ùå P1 Calc Error: {e}", exc_info=True)

    logger.info("\n--- Testing P3 loss/acc calc ---")
    try:
        _seq_len = 10
        _vocab_size = 13
        PAD_TOKEN_ID = 0  # Assume PAD=0 for test
        dummy_model_p3 = DummyP3Model(vocab_size=_vocab_size, embed_dim=32)
        mx.eval(dummy_model_p3.parameters())

        # Data: List[PIL], full_target_sequence (MLX)
        # Removed dummy_images_p3_pt definition, will use torch.randn directly below

        dummy_labels_p3 = mx.array(
            np.random.randint(0, _vocab_size, (dummy_batch_size, _seq_len))
        )

        # --- üëá Correct way to SET values using standard slicing ---
        slice_shape = dummy_labels_p3[:, -2:].shape
        pad_values = mx.full(
            slice_shape, PAD_TOKEN_ID, dtype=dummy_labels_p3.dtype
        )
        dummy_labels_p3[:, -2:] = pad_values
        mx.eval(dummy_labels_p3)
        # --- End Correction ---

        logger.info(f"P3 Dummy Labels (with padding): {dummy_labels_p3}")

        loss, acc = calculate_loss_acc_mlx(
            dummy_model_p3,
            # Need to simulate PT tensor for images for current calc func
            torch.randn(
                dummy_batch_size, 3, 64, 64
            ),  # Using torch.randn directly
            dummy_labels_p3,
            phase=3,
            pad_token_id=PAD_TOKEN_ID,
            is_encoder_decoder=True,
        )
        mx.eval(loss, acc)
        logger.info(
            f"P3 Dummy Loss: {loss.item():.4f}, Acc: {acc.item()*100:.2f}%"
        )
        logger.info("‚úÖ P3 calc OK.")

    except Exception as e:
        logger.error(f"‚ùå P3 Calc Error: {e}", exc_info=True)

    logger.info("\n‚úÖ trainer_mlx.py basic checks finished.")
