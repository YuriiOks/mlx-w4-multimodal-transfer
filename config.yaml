# Multimodal Transfer Learning / Image Captioning - Configuration
# Copyright (c) 2025 Dropout Disco Team (Yurii, Artemis, Nnamdi, Kenton)
# File: config.yaml
# Description: Central configuration file.
# Created: 2025-05-05
# Updated: 2025-05-05

# --- General Paths ---
paths:
  model_save_dir: "models/image_captioning" # Example subdir for task
  log_dir: "logs"
  log_file_name: "multimodal_train.log"
  data_dir: "data" # Default location for downloaded datasets

# --- Tokenizer Configuration ---
tokenizer:
  hf_name: "openai/clip-vit-base-patch32" # Example: Use CLIP's tokenizer
  # Or: "gpt2", "bert-base-uncased" etc.
  pad_token_id: 0 # Check tokenizer.pad_token_id after loading
  start_token_id: 1 # Define custom or use tokenizer.bos_token_id
  end_token_id: 2   # Define custom or use tokenizer.eos_token_id
  vocab_size: 13  # Placeholder - get this from loaded tokenizer
  max_seq_len: 50 # Max caption length to generate/pad to

# --- Dataset Parameters ---
dataset:
  name: "flickr30k" # Example: 'flickr30k', 'COCO_mini_val_captions' (use HF dataset name)
  image_size: 224    # Input size expected by ViT/CLIP
  patch_size: 16     # Example for ViT-Base-Patch16
  # Phase specific params not strictly needed if handling via task/model config

# --- Vision Encoder Configuration ---
encoder:
  hf_name: "google/vit-base-patch16-224-in21k" # Example ViT
  # Or: "openai/clip-vit-base-patch32" # Example CLIP vision tower
  output_layer: -1 # Which layer's features to extract (-1 is usually last hidden state)
  # Add flags for freezing etc. if needed

# --- Decoder Model Hyperparameters ---
model: # Parameters for the custom decoder
  embed_dim: 768       # Should match encoder output dimension if directly connected
  depth: 6             # Number of decoder blocks
  num_heads: 8         # Number of attention heads
  mlp_ratio: 4.0       # Decoder MLP expansion ratio
  dropout: 0.1
  attention_dropout: 0.1

# --- Training Hyperparameters ---
training:
  # Define ONE set of params initially, can add phase-specific later if needed
  epochs: 20
  batch_size: 64
  base_lr: 5e-5       # Lower LR common for fine-tuning / transfer learning
  weight_decay: 0.01
  warmup_steps: 500    # Example: Use steps instead of epochs for warmup
  optimizer: "AdamW"
  scheduler: "CosineDecayWithWarmup" # Example
  gradient_clipping: 1.0
  # num_train_samples: -1 # Use -1 for full dataset, or set number
  # num_val_samples: -1
  save_every: 1        # Save checkpoint every N epochs
  eval_every: 1        # Evaluate every N epochs

# --- Evaluation Parameters ---
evaluation:
  batch_size: 128        # Can often be larger than training batch size
  # Add metric calculation settings if needed

# --- Inference Parameters ---
inference:
  generation_method: "greedy" # or "beam"
  beam_size: 5              # if using beam search
  max_gen_len: 50           # Max tokens to generate

# --- Logging Configuration ---
logging:
  log_level: "INFO"
  log_file_enabled: True
  log_console_enabled: True
  log_max_bytes: 10485760
  log_backup_count: 5
