# Multimodal Transfer Learning / Image Captioning - Configuration
# File: config.yaml
# Copyright (c) 2025 Dropout Disco Team (Yurii, Artemis, Nnamdi, Kenton)
# Description: Central configuration file.
# Created: 2025-05-05
# Updated: 2025-05-07

# --- General Paths ---
paths:
  model_save_dir: "models/image_captioning"
  log_dir: "logs"
  log_file_name: "multimodal_train.log"
  data_dir: "data"

# --- Tokenizer Configuration ---
tokenizer:
  hf_name: "gpt2" # Using GPT2 tokenizer as a base
  pad_token_id: 50257 # Will be updated by tokenizer init
  start_token_id: 50258
  end_token_id: 50259
  vocab_size: 50261 # Will be updated by tokenizer init
  max_seq_len: 50

# --- Dataset Parameters ---
dataset:
  name: "HuggingFaceM4/COCO_mini_val" # Or your chosen dataset
  hf_config_name: null # If dataset has specific configurations
  caption_col: "captions" # Adjust based on dataset
  image_col: "image"    # Adjust based on dataset
  image_size: 224
  # patch_size not directly used by dataset if using HF processor

# --- Vision Encoder Configuration ---
encoder:
  hf_name: "google/vit-base-patch16-224-in21k"
  # For ViT, last_hidden_state feature dim is usually model.config.hidden_size
  # For CLIP vision tower, feature dim is model.config.hidden_size
  feature_dim: 768 # Explicitly state this for clarity
  freeze: True

# --- Decoder Model Hyperparameters ---
model: # Parameters for the custom PyTorch/MLX decoder
  # This embed_dim is for the DECODER part (text embeddings, decoder layers)
  # Make it match the encoder's output feature_dim for direct connection
  decoder_embed_dim: 768   # <--- Key Change: Match encoder.feature_dim
  decoder_depth: 3         # Number of decoder blocks (as in your test)
  decoder_num_heads: 8     # Number of attention heads in decoder
  decoder_ffn_dim_ratio: 4.0 # Decoder MLP expansion ratio (ffn_dim = embed_dim * ratio)
  dropout: 0.1

# --- Training Hyperparameters ---
training:
  epochs: 20
  batch_size: 32 # Might need to be small due to model size
  base_lr: 1e-4  # Common starting LR for fine-tuning decoders
  weight_decay: 0.01
  optimizer: "AdamW"
  # Optional scheduler params
  # scheduler: "CosineDecayWithWarmup"
  # warmup_steps: 500
  gradient_clipping: 1.0
  save_every: 1
  eval_every: 1

# --- Evaluation Parameters ---
evaluation:
  batch_size: 64

# --- Inference Parameters ---
inference:
  generation_method: "greedy"
  beam_size: 5
  max_gen_len: 50

# --- Logging Configuration ---
logging:
  log_level: "INFO"
  log_file_enabled: True
  log_console_enabled: True
  log_max_bytes: 10485760
  log_backup_count: 5
