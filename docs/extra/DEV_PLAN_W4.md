# Week 4 Development Plan:

**Team:** Dropout Disco (Yurii, Artemis, Kenton, Nnamdi)
**Goal:**

---
**Overall Plan:**

---
**Detailed Action Plan Log & To-Do List:**

| Day         | Time Block | Focus Area         | Specific Action(s)                                                                                                                                                                                                                                                                                                                                | File(s) Involved (Primary)                                                                                                                           | Goal / Output ‚úîÔ∏è                                                                                                                                                                           | Status      | Assignee(s) |
| :---------- | :--------- | :----------------- | :------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | :--------------------------------------------------------------------------------------------------------------------------------------------------- | :----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | :------------ | :---------- |
| **Day 1 (Mon)** | **PM**     | ‚úÖ **Setup & Prep**  | 1. (DONE) Project setup, env, config, docs, MLX install, branch.                                                                                                                                                                                                                                                                           | Project Root, `config.yaml`, `requirements.txt`, etc.                                                                                                | Base project ready.                                                                                                                                                      | ‚úÖ Done         | Team        |
| **Day 2 (Tue)** | ---        | ‚è∏Ô∏è **SKIP DAY**      | *(No development)*                                                                                                                                                                                                                                                                                                                                 | ---                                                                                                                                                  | ---                                                                                                                                                                                      | ---           | ---         |
| **Day 3 (Wed)** | **Early AM** | üìä **Data (MLX)**    | 1. **Dataset Choice & HF Load Test:** Confirm dataset (Flickr30k). In `notebooks/01_data_exploration.ipynb`, load a few samples using `datasets.load_dataset()` to confirm access and structure. <br> 2. **Tokenizer Setup:** In `src/common/tokenizer.py`, implement `init_tokenizer` (e.g., using `AutoTokenizer.from_pretrained("gpt2")` or `CLIPTokenizer`), add special tokens (`<pad>`, `<start>`, `<end>`). Define functions `tokenize_captions` and `decode_tokens`. Test in notebook. <br> 3. **Constants:** Define `PAD_TOKEN_ID`, `START_TOKEN_ID`, `END_TOKEN_ID`, `VOCAB_SIZE` in `src/common/constants.py` based on tokenizer. | `notebooks/01_...ipynb`, `src/common/tokenizer.py`, `src/common/constants.py`                                                                        | Dataset access confirmed. Text tokenizer ready & tested. Special token IDs defined.                                                                                      | ‚è≥ **NEXT**   | Team        |
| Day 3 (Wed) | AM         | üñºÔ∏è **Encoder (MLX)** | 1. **Implement `src/models/encoder_wrapper.py`:** <br>   a. `load_vision_encoder(model_name, framework)`: Loads ViT/CLIP using `transformers` if `framework='pt'`, or loads MLX-equivalent if one exists (if not, plan to convert PT features). <br>   b. `get_image_features(encoder, processor, image_list, framework, target_device_for_mlx_conversion)`: Preprocesses, runs encoder. **Critically: If PT encoder, convert output tensor to `mx.array` and move to `mx.default_device()` (GPU).** <br> 2. **Test:** In `notebooks/02_encoder_testing.ipynb`, load PT ViT/CLIP, get features, convert to `mx.array`, check shape/dtype. | `src/models/encoder_wrapper.py`, `notebooks/02_...ipynb`                                                                                             | Function to get image features as MLX arrays, handling PT->MLX conversion.                                                                                               | ‚è≥ To Do       | Team        |
| Day 3 (Wed) | Mid-AM     | üß± **Modules (MLX)** | 1. **Implement `src/models/mlx/modules.py`:** <br>   a. `AttentionMLX(embed_dim, num_heads, dropout)`: Implement MHA from scratch (separate Q,K,V linear projections per head, concat, final projection) supporting `mask` argument. <br>   b. `MLPBlockMLX(embed_dim, mlp_ratio, dropout)`. <br>   c. `TransformerDecoderBlockMLX(embed_dim, num_heads, mlp_ratio, dropout)`: Use `AttentionMLX` for masked self-attn & cross-attn, and `MLPBlockMLX`. <br> 2. Add simple shape tests in `if __name__ == "__main__":`. | `src/models/mlx/modules.py`                                                                                                                            | Core MLX Decoder components implemented & shape-tested.                                                                                                  | ‚è≥ To Do       | MLX Team    |
| Day 3 (Wed) | Late AM    | üèóÔ∏è **Model (MLX)**   | 1. **Implement `src/models/mlx/caption_model.py`:** <br>   a. `ImageCaptionerMLX(mlx.nn.Module)`. <br>   b. Init `encoder_wrapper` (to get image features). <br>   c. Init `mlx.nn.Embedding` for decoder target tokens (use `VOCAB_SIZE` from `constants.py`). <br>   d. Init Decoder Positional Embeddings (`mx.array`). <br>   e. Init stack of `TransformerDecoderBlockMLX`. <br>   f. Init final `mlx.nn.Linear` output head (to `VOCAB_SIZE`). <br> 2. Implement `__call__(self, image_input_for_encoder, decoder_tgt_sequence_tokens)`. Ensure causal mask for decoder. <br> 3. Add shape tests in `if __name__ == "__main__":`. | `src/models/mlx/caption_model.py`                                                                                                                      | Full MLX Encoder-Decoder (using wrapper for enc) model defined.                                                                                                        | ‚è≥ To Do       | MLX Team    |
| Day 3 (Wed) | **Early PM** | üíæ **Data (MLX)**    | 1. **Implement `src/data/image_datasets.py`:** <br>   a. Create `ImageCaptionDatasetMLX(base_hf_dataset, tokenizer, image_processor, max_seq_len)`: <br>     i. `__init__`: Stores HF dataset, tokenizer, processor. <br>     ii. `__len__`. <br>     iii. `__getitem__`: Loads image (PIL), tokenizes caption, pads caption sequence, **returns (image_pil, caption_ids_np, attention_mask_np)**. (Keep as NumPy here for batching). <br> 2. **Implement `src/data/dataloader_mlx.py` (manual batching):** <br>   a. Function `create_mlx_batches(dataset, batch_size, shuffle=True)`: Takes `ImageCaptionDatasetMLX`, yields batches of `(list_of_pil_images, batched_caption_ids_np, batched_attn_mask_np)`. | `src/data/image_datasets.py`, `src/data/dataloader_mlx.py` (*New*)                                                                                       | Dataset class providing (PIL image, NumPy tokenized caption). Manual batch iterator for MLX.                                                                     | ‚è≥ To Do       | Team        |
| Day 3 (Wed) | PM         | üöÇ **Trainer (MLX)** | 1. **Implement `src/training/mlx/trainer.py`:** <br>   a. `calculate_loss_acc_mlx(model, image_features_mlx, decoder_input_mlx, decoder_target_mlx, pad_token_id)`: Runs decoder, calculates sequence CrossEntropy (ignore PAD), accuracy (ignore PAD). <br>   b. `loss_and_grad_fn = mlx.nn.value_and_grad(model.decoder, calculate_loss_acc_for_grad)` (Note: grad only on decoder part). <br>   c. `train_epoch_mlx`: Loop through batches. Inside loop: <br>     i. Get image features via `encoder_wrapper.get_image_features(batch_pil_images, framework='mlx')`. <br>     ii. Prepare `decoder_input_mlx`, `decoder_target_mlx` from `batch_caption_ids_mlx`. <br>     iii. Call `loss_and_grad_fn`. <br>     iv. `optimizer.update(model.decoder, grads)`. <br>     v. `mx.eval()`. <br>   d. `evaluate_model_mlx`. <br> e. `train_model_mlx` orchestrator. | `src/training/mlx/trainer.py`                                                                                                                        | MLX training/evaluation loops capable of training the decoder part.                                                                                              | ‚è≥ To Do       | MLX Team    |
| Day 3 (Wed) | Late PM    | ‚öôÔ∏è **Checkpoint(MLX)**| 1. **Implement `src/training/mlx/checkpoint.py`:** <br>   a. `save_checkpoint_mlx(model_decoder, optimizer, epoch, metrics, save_dir, model_config, dataset_config, phase)`. Saves `decoder.save_weights()`, optimizer state (if possible), other data as `.pkl`, configs as `.yaml`. <br>   b. `load_checkpoint_mlx(save_dir)` returns components. <br> 2. Integrate into `train_model_mlx`. | `src/training/mlx/checkpoint.py` (*New*)                                                                                                             | Robust MLX checkpointing for decoder.                                                                                                                              | ‚è≥ To Do       | MLX Team    |
| Day 3 (Wed) | EOD        | üöÄ **Script (MLX)**  | 1. **Implement/Refine `scripts/run_training.py` for `--framework mlx`:** <br>   a. Load data using `ImageCaptionDatasetMLX` and `create_mlx_batches`. <br>   b. Instantiate `ImageCaptionerMLX` (use loaded config for arch if resuming). <br>   c. Create MLX Optimizer. <br> d. Implement `--resume` logic. <br>   e. Call `train_model_mlx`. <br> 2. **Run short MLX training (1-2 epochs). Debug.** | `scripts/run_training.py`, `config.yaml`                                                                                                               | End-to-end MLX training pipeline for captioning runs.                                                                                                              | ‚è≥ To Do       | Team        |
| **Day 4 (Thu)** | **AM**     | üî• **MLX Train**     | 1. Run longer MLX training (e.g., 5-10 epochs, or more if fast). <br> 2. Monitor W&B. Aim for decreasing loss and increasing (basic) accuracy. Don't expect SOTA yet. | Console Output, W&B Run                                                                                                                                | Trained MLX decoder model checkpoint saved.                                                                                                                        | ‚è≥ To Do       | Team        |
| Day 4 (Thu) | AM         | ‚ú® **Inference(MLX)**| 1. **Implement `src/inference/generator_mlx.py`:** <br>   a. `generate_caption_mlx(encoder_wrapper, decoder_model, image_processor, tokenizer, image, max_len, start_id, end_id, pad_id)`: Greedy decoding loop. <br>   b. (Optional) Beam Search. <br> 2. Test in `notebooks/04_inference_example.ipynb`. | `src/inference/generator_mlx.py`, `notebooks/04_...ipynb`                                                                                          | Function to generate captions with trained MLX model.                                                                                                              | ‚è≥ To Do       | MLX Team    |
| Day 4 (Thu) | **PM**     | üö¢ **Streamlit App** | 1. **Implement `app/model_loader.py`:** `load_selected_model` to load chosen *MLX decoder weights* and *encoder_wrapper*, using *saved config* from checkpoint to instantiate model. <br> 2. **Implement `app/app.py` + submodules:** Connect UI (upload/draw), preprocessing, model loading, call `generate_caption_mlx`, display caption. | `app/`                                                                                                                                                 | Streamlit app runs locally, loads MLX model, generates captions.                                                                                                 | ‚è≥ To Do       | Team        |
| Day 4 (Thu) | Late PM    | üê≥ **Docker**        | 1. **Finalize `Dockerfile`:** Base Python image. Copy app, src, utils, models (MLX only). Install `requirements.txt`. `ENTRYPOINT ["streamlit", "run", "app/app.py"]`. <br> 2. **Build & Test:** `docker build -t multimodal-app .` then `docker run -p 8501:8501 multimodal-app`. | `Dockerfile`, `.dockerignore`                                                                                                                          | Docker container runs the Streamlit app.                                                                                                                         | ‚è≥ To Do       | Team        |
| Day 4 (Thu) | EOD        | üéÅ **Finalize**      | 1. Code cleanup, comments. <br> 2. Update `README.md`, `STRUCTURE.MD`, `DEV_PLAN_W4.md`. <br> 3. Prepare presentation talking points for MLX journey & demo. <br> 4. Final commits, push branch. | Docs, Presentation, Git Repo                                                                                                                           | Project deliverables ready.                                                                                                                                      | ‚è≥ To Do       | Team        |
