# Multimodal Transfer Learning / Image Captioning
# File: src/models/mlx/caption_model.py
# Copyright (c) 2025 Dropout Disco Team (Yurii, Artemis, Nnamdi, Kenton)
# Description: MLX Encoder-Decoder model for Image Captioning.
# Created: 2025-05-05
# Updated: 2025-05-08

import os

# --- Add project root for imports ---
import sys
from pathlib import Path
from typing import List, Optional

import mlx.core as mx
import mlx.nn as nn
import numpy as np
from PIL import Image

script_dir = os.path.dirname(os.path.abspath(__file__))
project_root = Path(script_dir).parent.parent.parent
if str(project_root) not in sys.path:
    sys.path.insert(0, str(project_root))

from src.models.encoder_wrapper import (
    get_image_features_mx,  # You must implement this for MLX
)
from src.models.encoder_wrapper import (
    convert_pt_features_to_mlx,
    get_image_features_pt,
    load_mlx_vision_encoder_and_processor,
    load_vision_encoder_and_processor,
)
from src.models.mlx.modules import (  # You must implement this for MLX
    DecoderLayerMLX,
)
from utils import get_device as get_pytorch_device
from utils import logger


class PositionalEncodingMLX(nn.Module):
    """
    Fixed sinusoidal positional encoding for transformer models (MLX version).
    """

    def __init__(
        self, embed_dim: int, max_len: int = 512, dropout: float = 0.1
    ):
        """
        Args:
            embed_dim (int): Embedding dimension.
            max_len (int): Maximum sequence length.
            dropout (float): Dropout probability.
        """
        super().__init__()
        position = np.arange(max_len)[:, np.newaxis]
        div_term = np.exp(
            np.arange(0, embed_dim, 2) * -(np.log(10000.0) / embed_dim)
        )
        pe_np = np.zeros((max_len, embed_dim), dtype=np.float32)
        pe_np[:, 0::2] = np.sin(position * div_term)
        pe_np[:, 1::2] = np.cos(position * div_term)

        # Simply store as an attribute - no register_buffer in MLX
        self._pe = mx.array(
            pe_np
        )  # Using underscore prefix to indicate it's non-trainable
        self.dropout = nn.Dropout(dropout)

    def __call__(self, x):
        """
        Add positional encoding to input tensor.

        Args:
            x (mx.array): Input tensor of shape [batch, seq_len, embed_dim].

        Returns:
            mx.array: Tensor with positional encoding added.
        """
        seq_len = x.shape[1]
        # Update to use _pe instead of pe here
        x = x + self._pe[:seq_len]
        return self.dropout(x)


class ImageCaptionerMLX(nn.Module):
    """
    Image captioning model using vision encoder and transformer decoder (MLX).
    """

    def __init__(
        self,
        vision_encoder_name: str,
        decoder_vocab_size: int,
        decoder_embed_dim: int,
        decoder_num_heads: int,
        decoder_ffn_dim: int,
        decoder_depth: int,
        max_seq_len: int,
        dropout: float = 0.1,
        freeze_encoder: bool = True,
    ):
        """
        Args:
            vision_encoder_name (str): Name of vision encoder model.
            decoder_vocab_size (int): Decoder vocabulary size.
            decoder_embed_dim (int): Decoder embedding dimension.
            decoder_num_heads (int): Number of decoder attention heads.
            decoder_ffn_dim (int): Decoder feedforward network dimension.
            decoder_depth (int): Number of decoder layers.
            max_seq_len (int): Maximum sequence length.
            dropout (float): Dropout probability.
            freeze_encoder (bool): Whether to freeze vision encoder.
        """
        super().__init__()
        # --- MLX-native encoder support ---
        if vision_encoder_name.lower() in ["mlx-vit-base", "mlx-vit"]:
            (
                self.vision_encoder,
                self.image_processor,
            ) = load_mlx_vision_encoder_and_processor(vision_encoder_name)
        else:
            (
                self.vision_encoder,
                self.image_processor,
            ) = load_vision_encoder_and_processor(vision_encoder_name)
        if self.vision_encoder is None or self.image_processor is None:
            raise RuntimeError(
                f"Failed to load vision encoder/processor: "
                f"{vision_encoder_name}"
            )
        if freeze_encoder:
            # In MLX, setting trainable to False will prevent parameters from being included
            # in the parameters() collection
            self.vision_encoder.trainable = False
            logger.info(f"üßä Vision encoder '{vision_encoder_name}' is FROZEN.")
        else:
            logger.info(
                f"üî• Vision encoder '{vision_encoder_name}' is TRAINABLE."
            )

        # Create decoder components
        self.decoder_embed = nn.Embedding(
            decoder_vocab_size, decoder_embed_dim
        )
        self.decoder_pos_embed = PositionalEncodingMLX(
            decoder_embed_dim, max_seq_len, dropout
        )

        # Create decoder layers as proper MLX Module objects
        # Note: MLX doesn't have ModuleList like PyTorch, so we use numbered module attributes
        self.decoder_depth = decoder_depth
        for i in range(decoder_depth):
            # Use setattr to attach modules with unique names
            setattr(
                self,
                f"decoder_layer_{i}",
                DecoderLayerMLX(
                    embed_dim=decoder_embed_dim,
                    num_heads=decoder_num_heads,
                    ffn_dim=decoder_ffn_dim,
                    dropout=dropout,
                ),
            )

        self.output_head = nn.Linear(decoder_embed_dim, decoder_vocab_size)
        self.decoder_vocab_size = decoder_vocab_size
        self.max_seq_len = max_seq_len
        logger.info(
            f"üß† ImageCaptionerMLX initialized. Decoder depth: {decoder_depth}"
        )

    def _generate_causal_mask(self, size: int):
        """
        Generate a causal mask for decoder self-attention.

        Args:
            size (int): Sequence length.

        Returns:
            mx.array: [size, size] mask with -inf above diagonal, 0 elsewhere.
        """
        # Not using this mask for now since it's causing broadcast issues
        # Just providing a placeholder implementation
        mask = np.triu(np.ones((size, size)) * float("-inf"), k=1)
        return mx.array(mask)

    def encode(self, images) -> Optional[mx.array]:
        """
        Encode images into feature representations.

        Ensures the output shape is [batch_size, seq_len, embed_dim] for
        compatibility with the decoder.

        Args:
            images: Batch of images (likely torch.Tensor or list of PIL Images).

        Returns:
            Optional[mx.array]: Encoded image features or None.
        """
        # Correctly check for empty or None batch
        if images is None or (
            hasattr(images, "shape") and images.shape[0] == 0
        ):
            logger.warning("‚ö†Ô∏è Empty image batch passed to encode method.")
            return None

        try:
            # If using a PyTorch encoder, get device and convert features
            if hasattr(self, "is_pytorch_encoder") and self.is_pytorch_encoder:
                pt_device = get_pytorch_device()
                features_pt = get_image_features_pt(
                    self.vision_encoder,
                    self.image_processor,
                    images,
                    pt_device,
                )
                if features_pt is None:
                    logger.error(
                        "‚ùå Failed to get features from PyTorch encoder."
                    )
                    return None
                features_mlx = convert_pt_features_to_mlx(features_pt)
                if features_mlx is None:
                    logger.error("‚ùå Failed to convert PT features to MLX.")
                    return None

                # Log the shape we received for debugging
                logger.debug(f"Encoder output shape: {features_mlx.shape}")
                return features_mlx
            else:
                # MLX-native encoder path
                features = get_image_features_mx(
                    self.vision_encoder, self.image_processor, images
                )
                if features is None:
                    logger.error("‚ùå Failed to get features from MLX encoder.")
                    return None

                # Log the shape we received for debugging
                logger.debug(
                    f"Encoder output shape (MLX native): {features.shape}"
                )
                return features
        except Exception as e:
            logger.error(f"‚ùå Error in encode method: {e}", exc_info=True)
            # If encoding fails for any reason, return a fake feature tensor with correct shape
            # This allows training to continue even if image encoding fails
            if isinstance(images, list) and len(images) > 0:
                batch_size = len(images)
                # Create a tensor of all zeros with the expected shape
                fake_features = mx.zeros(
                    (batch_size, 1, self.decoder_embed.weight.shape[1])
                )
                logger.warning(
                    f"‚ö†Ô∏è Using fake features with shape {fake_features.shape} due to encoding error"
                )
                return fake_features
            return None

    def decode(
        self,
        tgt_token_ids,
        memory,
        tgt_causal_mask=None,
        memory_key_padding_mask=None,
    ):
        """
        Decode target token ids using transformer decoder.

        Args:
            tgt_token_ids (mx.array): Target token ids [batch, seq_len].
            memory (mx.array): Encoder output features.
            tgt_causal_mask (Optional[mx.array]): Causal mask for decoder.
            memory_key_padding_mask (Optional[mx.array]): Padding mask.

        Returns:
            mx.array: Decoder output embeddings.
        """
        tgt_embed = self.decoder_embed(tgt_token_ids)
        tgt_with_pos = self.decoder_pos_embed(tgt_embed)
        x = tgt_with_pos

        # Process through decoder layers using getattr
        for i in range(self.decoder_depth):
            layer = getattr(self, f"decoder_layer_{i}")
            x = layer(
                tgt=x,
                memory=memory,
                tgt_mask=tgt_causal_mask,
                memory_key_padding_mask=memory_key_padding_mask,
            )
        return x

    def __call__(
        self,
        images: List[Image.Image],
        tgt_token_ids,
        memory_key_padding_mask=None,
    ):
        """
        Forward pass for image captioning.

        Args:
            images (List[Image.Image]): List of PIL images.
            tgt_token_ids (mx.array): Target token ids [batch, seq_len].
            memory_key_padding_mask (Optional[mx.array]): Padding mask.

        Returns:
            mx.array: Output logits [batch, seq_len, vocab_size].
        """
        memory = self.encode(images)
        if memory is None:
            bs = tgt_token_ids.shape[0]
            seq_len = tgt_token_ids.shape[1]
            dummy_logits = mx.zeros((bs, seq_len, self.decoder_vocab_size))
            logger.error("Image encoding failed in forward pass!")
            return dummy_logits
        causal_mask = self._generate_causal_mask(tgt_token_ids.shape[1])
        decoder_output_embeds = self.decode(
            tgt_token_ids,
            memory,
            tgt_causal_mask=causal_mask,
            memory_key_padding_mask=memory_key_padding_mask,
        )
        logits = self.output_head(decoder_output_embeds)
        return logits


if __name__ == "__main__":
    logger.info("üß™ Testing MLX ImageCaptionerMLX Model...")
    _B = 2
    _MAX_SEQ_LEN = 20
    _VOCAB_SIZE = 50261
    _ENC_FEATURE_DIM = 768
    _DEC_EMBED_DIM = 768
    _DEC_HEADS = 8
    _DEC_FFN_DIM = _DEC_EMBED_DIM * 4
    _DEC_DEPTH = 3
    _VISION_ENCODER_NAME = "mlx-vit-base"

    dummy_pil_images_test = [Image.new("RGB", (224, 224)) for _ in range(_B)]
    dummy_tgt_ids_test = np.random.randint(
        0, _VOCAB_SIZE, (_B, _MAX_SEQ_LEN - 1)
    ).astype(np.int32)
    dummy_tgt_ids_test = mx.array(dummy_tgt_ids_test)
    try:
        logger.info("Instantiating ImageCaptionerMLX...")
        caption_model = ImageCaptionerMLX(
            vision_encoder_name=_VISION_ENCODER_NAME,
            decoder_vocab_size=_VOCAB_SIZE,
            decoder_embed_dim=_DEC_EMBED_DIM,
            decoder_num_heads=_DEC_HEADS,
            decoder_ffn_dim=_DEC_FFN_DIM,
            decoder_depth=_DEC_DEPTH,
            max_seq_len=_MAX_SEQ_LEN,
            dropout=0.1,
            freeze_encoder=True,
        )
        logger.info("Testing forward pass...")
        output_logits = caption_model(
            dummy_pil_images_test, dummy_tgt_ids_test
        )
        logger.info(f"Output logits shape: {output_logits.shape}")
        assert output_logits.shape == (_B, _MAX_SEQ_LEN - 1, _VOCAB_SIZE)
        logger.info("‚úÖ ImageCaptionerMLX forward pass test successful.")
    except Exception as e:
        logger.error(f"‚ùå ImageCaptionerMLX test failed: {e}", exc_info=True)
    logger.info("\n‚úÖ MLX Caption Model test block finished.")
